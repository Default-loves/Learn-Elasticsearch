分布式搜索引擎
大数据近实时分析引擎
海量数据的分布式存储和集群管理

### Elasticsearch

- Beats：轻量的数据采集器
- Logstash：服务器端数据处理管道，从多个数据源采集数据，转换数据，并将数据发送到不同的存储库中
- Kibana：可视化
- Elasticsearch：存储和计算

### 应用场景
- 网站搜索、垂直搜索、代码搜索
- 日志管理和分析、Web抓取舆情分析、安全指标监控、应用性能监控

### 和关系数据库对比

- Index相当于RDBMS的Table

- Document相当于RDBMS的Row

- Field相当于RDBMS的Column

- Mapping相当于RDBMS的Schema

- DSL相当于RDBMS的SQL

相比关系型数据库，Elasticsearch提供了模糊查询，搜索条件的算分等功能

### 简单概念

- shard是物理空间的概念，索引中的数据分布在shard中

- Mapping定义文档字段的类型
- Settings定义不同的数据分布
- REST API能够被各种语言进行调用，通过发送HTTP Request给REST API，返回HTTP Response给调用者

### 命令
- Elasticsearch：`http://localhost:9200/`
- 查看插件：`elasticsearch-plugin list`
- 下载插件：`elasticsearch-plugin install analysis-icu`
- 查看插件情况：`localhost:9200/_cat/plugins`
- 运行多个Elasticsearch实例：
  elasticsearch -E node.namme=node0 -E cluster.name=junyi -E path.data=node0_data -d
  elasticsearch -E node.namme=node1 -E cluster.name=junyi -E path.data=node1_data -d
  elasticsearch -E node.namme=node2 -E cluster.name=junyi -E path.data=node2_data -d
  elasticsearch -E node.namme=node3 -E cluster.name=junyi -E path.data=node3_data -d


- 查看多个节点的情况：`http://localhost:9200/_cat/nodes`

- 上传文件到Elasticsearch：`logstash -f logstash.conf`


- cerebro：9000
- kibana：5601



### CRUD

#### Index

```http
//如果ID不存在会创建新的文档，如果存在会删除现有的文档，在创建新的文档，version号会增加
PUT my_index/_doc/1
{"user":"junyi", "comment":"OK"}
```

#### Create

```http
//如果ID已经存在会报错
PUT my_index/_create/1
{"user":"junyi", "comment":"OK"}

//不指定ID，自动生成
POST my_index/_doc
{"user":"junyi", "comment":"OK"}

```

#### Read

```http
GET my_index/_doc/1
```

#### Update

```http
//操作会对字段做增量修改
POST my_index/_update/1
{ "doc":{"field":"field"}}
```

#### Delete

```http
DELETE my_index/_doc/1
```



### Bulk

- 支持在一次HTTP请求中，对不同的索引进行操作（Index、Create、Update、Delete）
- 操作中单条语句失败不会影响其他语句的执行

```http
POST _bulk
{"index":{"_index":"test","_id":"1"}}
{"field1":"value1"}
{"delete":{"_index":"test","_id":"2"}}
{"create":{"_index":"test2","_id":"3"}}
{"field1":"value3"}
{"update":{"_index":"test","_id":"1"}}
{"doc":{"field2":"value2"}}
```

### mget-批量读取

```http
GET _mget
{
    "docs":[
        {
            "_index":"test",
            "_id":1
        },
        {
            "_index":"test2",
            "_id":3
        }
    ]
}
```

### msearch-批量查询

```http
POST users/_msearch
{}
{"query":{"match_all":{}}, "size":1}
{"index":{""}}
{"query":{"match_all":{}}, "size":1}

```

### 倒排索引

倒排索引包括两个部分：单词列表和倒排列表

- 单词列表一般比较大，为了实现高性能的插入和查询，可以使用的数据结构是B+树和哈希拉链法
- 倒排列表记录了单词对应的文档，由倒排索引项（ID、词频、位置、偏移）构成

Elasticsearch的JSON文档中的每个字段都有自己的倒排索引，也可以指定某些字段不做索引



### Analyzer

Analyzer由三部分组成：Character Filters、Tokenizer、Token Filters

- Character Filters：针对原始文本处理，例如去除html标签，字符串替换等
- Tokenizer：按照规则进行分词，例如根据空格进行分词
- Token Filters：将切分的单词进行加工，例如小写、去除停用词、增加同义词等

Elasticsearch内置的分词器：

- Standard Analyzer：默认分词器
- Simple Analyzer：
- Stop Analyzer
- Whitespace Analyzer
- Keyword Analyzer
- Patter Analyzer

```http
GET _analyze
{	
	"analyzer":"simple",
	"text":"2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}

GET _analyze
{
  "analyzer": "english",
  "text":"2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}

POST _analyze
{
  "analyzer": "standard",
  "text": "他说的确实有道理"
}

POST _analyze
{
  "analyzer": "icu_analyzer",
  "text": "他说的确实有道理"
}
```

```json
POST _analyze
{
	"char_filter":["html_strip"],
	"tokenizer":"keyword",
	"text":"<b>I'm fine.</b>"
}

POST _analyze
{
  "char_filter": [
      {
        "type":"mapping",
        "mappings":["- => _"]
      }
  ],
  "tokenizer": "standard",
  "text": ["1-2-3 12-89"]
}

POST _analyze
{
	"char_filter":[
		{
			"type":"pattern_replace",
			"pattern":"http://(.*)",
			"replacement":"$1"
		}
	],
	"tokenizer":"standard",
	"text":"http://www.google.com"
}

POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": ["D:/document/code/python"]
}

POST _analyze
{
	"tokenizer":"whitespace",
	"filter":["stop"],
	"text":["The rain in Spain falls mainly on the plain."]
}

//filter中两个的位置换一下效果不一样
POST _analyze
{
	"tokenizer":"whitespace",
	"filter":["lowercase", "stop"],
	"text":["The girls in China are playing this game.", "The rain in Spain falls mainly on the plain."]
}


//自定义analyzer
DELETE my_index
PUT my_index
{
	"settings": {
		"analysis": {
			"analyzer" : {
				"my_custom_analyzer": {
					"type":"custom",
					"char_filter":"emotions",
					"tokenizer":"punctuation",
					"filter":["lowercase", "english_stop"]
				}
			},
			"tokenizer": {
				"punctuation": {
					"type":"pattern",
					"pattern":"[ .,!?]"
				}
			},
			"char_filter": {
				"emotions": {
					"type":"mapping",
					"mappings":[
						":) => happy_",
						":( => sad_"
					]
				}
			},
			"filter": {
				"english_stop": {
					"type":"stop",
					"stopwords":"_english_"
				}
			}
		}
	}
}
POST my_index/_analyze
{
	"analyzer":"my_custom_analyzer",
	"text":"I'm a :) person, and you ?"
}
GET my_index
```

#### HanLP

<http://www.hanlp.com/>

<https://github.com/KennFalcon/elasticsearch-analysis-hanlp>

#### IK

<https://github.com/medcl/elasticsearch-analysis-ik>

#### pinyin

https://github.com/medcl/elasticsearch-analysis-pinyin/releases/download/v7.5.2/elasticsearch-analysis-pinyin-7.5.2.zip

