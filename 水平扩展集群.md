### 集群的部署方式

- 一个节点只承担一个角色，除了方便管理，也能够更高效利用硬件资源，比如Master只需要低配置的CPU，RAM和磁盘，而Data需要使用高配置的CPU，RAM和磁盘，Ingest使用高配置的CPU、中配置的RAM和低配置的磁盘
- 一般在生产环境中配置3个Dedicate Master Node
- 当磁盘容量或者磁盘读写压力比较大的时候，增加Data Node
- 当系统中有大量的复杂查询和聚合的时候，增加Coordinating Node
- 一般Application连接的是一个Load Balancer，负责负载均衡，再和Coordinating Node或者Ingest Node交互
- 读写分离：写LB与Ingest Node交互，读LB与Coordinating Node交互
- 将集群分布在多个数据中心



### Hot & Warm架构

- Hot 节点（通常使用SSD）：放置经常有更新的索引，使用配置较高的机器
- Warm节点（通常使用HDD）：放置不更新的索引，也不存在大量的数据查询，使用配置较低的机器

```json
//配置一个Hot节点
elasticsearch -E node.name=hotnode0 -E cluster.name=junyi -E path.data=hot_data -E http.port=9200 -E node.attr.my_node_type=hot
//配置一个Warm节点
elasticsearch -E node.name=warmnode0 -E cluster.name=junyi -E path.data=warm_data -E http.port=9201 -E node.attr.my_node_type=warm
//查看节点的情况
GET /_cat/nodeattrs?v
//创建索引的时候，让其创建在Hot节点上
PUT logs-2020-01
{
	"settings": {
		"number_of_shards": 2,
		"number_of_replicas": 0,
		"index.routing.allocation.require.my_node_type": "hot"
	}
}
//查看索引的情况
GET _cat/shards?v
//将索引移动到Warm节点
PUT logs-2020-01
{
	"index.routing.allocation.require.my_node_type": "warm"
}
```

#### Rack Awareness

对于多机架的集群，Rack Awareness机制能够避免将索引主副分片放置在一个机架上

```json
//标记一个rack1
elasticsearch -E node.name=node1 -E cluster.name=junyi -E path.data=node1_data -E http.port=9200 -E node.attr.my_rack_id=rack1
//标记一个rack2
elasticsearch -E node.name=node2 -E cluster.name=junyi -E path.data=node2_data -E http.port=9201 -E node.attr.my_rack_id=rack2
//配置集群
PUT _cluster/settings
{
	"persistent": {
		"cluster.routing.allocation.awareness.attributes": "my_rack_id"
	}
}

//当节点都配置在一个机架上的时候，可以使用Forced Awareness强制指定主副分片配置在多个机架
//都配置在rack1上
elasticsearch -E node.name=node1 -E cluster.name=junyi -E path.data=node1_data -E http.port=9200 -E node.attr.my_rack_id=rack1
elasticsearch -E node.name=node2 -E cluster.name=junyi -E path.data=node2_data -E http.port=9201 -E node.attr.my_rack_id=rack1
//集群配置
PUT _cluster/settings
{
	"persistent": {
		"cluster.routing.allocation.awareness.attributes": "my_rack_id",
		"cluster.routing.allocation.awareness.force.zone.value": "rack1, rack2"
	}
}
//当rack2不在的时候，集群显示yellow
GET _cluster/health
//副本无法分配
GET _cat/shards?v
//查看集群的具体情况
GET _cluster/allocation/explain?pretty
```

### 分片设计与管理

- Elasticsearch7.0开始，新创建的索引，默认只有一个主分片
- 对于多分片，查询能够并行执行，数据写入能够分散到多个机器
- 多分片的坏处：由于每个分片都是一个Lucene的索引，会使用机器资源，过多的分片会导致额外的性能开销（每次搜索的请求，需要从每个分片上获取数据；分片的meta信息需要由master Node维护，会增加管理的负担）

#### 确定主分片数

- 从存储的物理角度看
  - 日志类应用，单个分片的大小不要超过50GB
  - 搜索类应用，单个分片的大小不要超过20GB
- 为什么要控制分片存储大小
  - 提高update性能
  - merge的时候，减少所需要的资源
  - 丢失节点后，具备更快的恢复速度，能够更快得在集群内rebalancing

### 容量规划

做容量规划的时候，需要考虑的问题：

- 机器的软硬件配置
- 一条文档的大小、索引的总数据量、副本分片数
- 数据是如何写入的（Bulk的尺寸）
- 文档的复杂度，数据是如何读取的，数据的吞吐和性能需求（怎么样的查询和聚合）

硬件配置：

- 数据节点使用SSD
- 搜索等性能要求比较高的场景，使用SSD，按照1：10的比例配置内存和磁盘
- 日之类和查询并发低的场景，使用机械硬盘，按照1：50的比例配置内存和磁盘
- 单节点数据控制在2TB以下
- JVM配置机器内存的一半，JVM内存配置不超过32GB

拆分索引：

- 对于业务经常基于一个字段进行Filter，而且该字段又是一个数量有限的枚举值，例如订单所在的地区
- 单个索引有大量的数据，通过拆分索引，可以提高查询性能
- 如果业务上有大量的查询是基于一个字段进行Filter，但是该字段的数值并不固定，可以启用routing功能，按照filter字段的值分布到集群不同的shard，避免搜索全部的shard，提高性能

具体场景：

- 搜索类：固定大小的数据集，更关心搜索和读取的性能，可以增加副本分片数，提高查询的吞吐量

- 日志类：数据大小随着时间的增长而不断增加，需要结合Warm Node做数据老化处理，更关心数据写入的性能，索引使用time-based进行索引（在索引的名字中增加时间信息；可以使用Date Math）

### 在私有云上管理Elasticsearch集群

管理单集群存在的问题：手工增加删除节点、当节点挂掉的时候手工修复或者更换、集群版本变更、数据备份、迭代升级

- 可以使用Elastic Cloud Enterprise管理多个集群
- 可以基于容器技术，使用Operator模式进行编排管理（每个集群都有一个spec描述的配置文件，Operator程序监控了Kubernate API Server，当配置文件出现变化的时候，调用相应的API进行操作）

### 在公有云上部署管理Elasticsearch集群

- Elastic Cloud
- 阿里云

